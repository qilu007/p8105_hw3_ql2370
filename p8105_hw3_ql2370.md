p8105\_hw3\_ql2370
================
QiLu
10/12/2019

``` r
library(tidyverse)
```

    ## ── Attaching packages ───────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.2.1     ✔ purrr   0.3.2
    ## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

## Problem 1

``` r
library(p8105.datasets)
data("instacart")
janitor::clean_names(instacart)
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # … with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

Instacart is an online grocery service that allows you to shop online
from local stores. In New York City, partner stores include Whole Foods,
Fairway, and The Food Emporium. Instacart offers same-day delivery, and
items that users purchase are delivered within 2 hours.

This dataset describes the order history of “The Instacart Online
Grocery Shopping 2017”, which has 1384617 objects and 15 variables
including order id, product id, user id, reordered, days since prior
order, department id, etc.

For example, the order with order id 1 added totally 8 items which 4 of
them were reordered, and this order was placed on Thursday 10 am. The
same user placed the prior order 9 days ago. Also, the detailed products
information was included in this dataset.

``` r
aisle_rank = instacart %>%
  count(aisle, name = "n_aisle") %>% 
  mutate(rank = min_rank(desc(n_aisle)))

aisle_max = aisle_rank %>% 
  filter(rank(desc(n_aisle)) == 1)

aisle_size = count(aisle_rank)
aisle_size
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   134

``` r
aisle_max
```

    ## # A tibble: 1 x 3
    ##   aisle            n_aisle  rank
    ##   <chr>              <int> <int>
    ## 1 fresh vegetables  150609     1

1)  There were 134 aisles, and the most itmes ordered from “fresh
    vegetables”.

<!-- end list -->

``` r
aisle_plot = aisle_rank %>% 
  filter(n_aisle > 10000)

ggplot(aisle_plot, aes(x= aisle, y = n_aisle, color = aisle)) + 
  geom_point() +
   theme(legend.position = "bottom")
```

![](p8105_hw3_ql2370_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

2)  This is the plot that shows the number of items ordered in each
    aisle, limiting this to aisles with more than 10000 items ordered.

<!-- end list -->

``` r
baking_data = instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  mutate(rank = rank(desc(count))) %>% 
  filter(rank <= 3)

baking_data
```

    ## # A tibble: 9 x 4
    ## # Groups:   aisle [3]
    ##   aisle                  product_name                           count  rank
    ##   <chr>                  <chr>                                  <int> <dbl>
    ## 1 baking ingredients     Cane Sugar                               336     3
    ## 2 baking ingredients     Light Brown Sugar                        499     1
    ## 3 baking ingredients     Pure Baking Soda                         387     2
    ## 4 dog food care          Organix Chicken & Brown Rice Recipe       28     2
    ## 5 dog food care          Small Dog Biscuits                        26     3
    ## 6 dog food care          Snack Sticks Chicken & Rice Recipe Do…    30     1
    ## 7 packaged vegetables f… Organic Baby Spinach                    9784     1
    ## 8 packaged vegetables f… Organic Blueberries                     4966     3
    ## 9 packaged vegetables f… Organic Raspberries                     5546     2

3)  This is the table showing the three most popular items in each of
    the aisles “baking ingredients”, “dog food care”, and “packaged
    vegetables fruits”. Include the number of times each item is ordered
    in your table.

<!-- end list -->

``` r
apple_ice_cream_data = instacart %>% 
  filter(product_name =="Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean
  )
  
apple_ice_cream_data
```

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

4)  This is the table showing the mean hour of the day at which Pink
    Lady Apples and Coffee Ice Cream are ordered on each day of the
    week.

## Problem 2

``` r
library(p8105.datasets)
data("brfss_smart2010")
janitor::clean_names(brfss_smart2010)
```

    ## # A tibble: 134,203 x 23
    ##     year locationabbr locationdesc class topic question response
    ##    <int> <chr>        <chr>        <chr> <chr> <chr>    <chr>   
    ##  1  2010 AL           AL - Jeffer… Heal… Over… How is … Excelle…
    ##  2  2010 AL           AL - Jeffer… Heal… Over… How is … Very go…
    ##  3  2010 AL           AL - Jeffer… Heal… Over… How is … Good    
    ##  4  2010 AL           AL - Jeffer… Heal… Over… How is … Fair    
    ##  5  2010 AL           AL - Jeffer… Heal… Over… How is … Poor    
    ##  6  2010 AL           AL - Jeffer… Heal… Fair… Health … Good or…
    ##  7  2010 AL           AL - Jeffer… Heal… Fair… Health … Fair or…
    ##  8  2010 AL           AL - Jeffer… Heal… Heal… Do you … Yes     
    ##  9  2010 AL           AL - Jeffer… Heal… Heal… Do you … No      
    ## 10  2010 AL           AL - Jeffer… Heal… Unde… Adults … Yes     
    ## # … with 134,193 more rows, and 16 more variables: sample_size <int>,
    ## #   data_value <dbl>, confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>,
    ## #   data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

``` r
brfss_smart2010 = brfss_smart2010 %>% 
  filter(Topic == "Overall Health")

response_level = c("Poor","Good", "Fair", "Very good", "Excellent")
brfss_smart2010$Response = ordered(brfss_smart2010$Response, response_level)
```

``` r
more_than_7_2002 = brfss_smart2010 %>% 
  filter(Year == "2002") %>% 
  group_by(Locationabbr) %>% 
  summarize(n = n()) %>% 
  filter(n > 7)
```

In 2002, those were states observed at 7 or more locations.

``` r
more_than_7_2010 = brfss_smart2010 %>% 
  filter(Year == "2010") %>% 
  group_by(Locationabbr) %>% 
  summarize(n = n()) %>% 
  filter(n > 7)
```

In 2010, those were states observed at 7 or more locations.

``` r
excellent_resp = brfss_smart2010 %>% 
  filter(Response == "Excellent") %>% 
  group_by(Year, Locationabbr) %>% 
  summarise( mean = mean(Data_value)) %>% 
  drop_na()

excellent_resp %>% 
  ggplot(aes(x = Year, y = mean, color = Locationabbr)) + 
    geom_point() + geom_line() + 
    theme(legend.position = "bottom")
```

![](p8105_hw3_ql2370_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

This is the plot of this average value over time within a state.

``` r
NY_data_value = brfss_smart2010 %>% 
  filter(Year == "2006" | Year == "2010" ) %>% 
  filter(Locationabbr == "NY") 

NY_data_value %>% 
  ggplot(aes(x = Response, y = Data_value)) +
  geom_boxplot() +
  facet_grid(~Year)
```

![](p8105_hw3_ql2370_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->
This is the two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

## Problem 3

``` r
library(dplyr)
sheet_data = read_csv("./data/accel_data.csv")
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
sheet_data = janitor::clean_names(sheet_data)



sheet_data["week_or_not"] <- "week"
sheet_data$week_or_not[sheet_data$day == "Saturday"] <- "weekend"
sheet_data$week_or_not[sheet_data$day == "Sunday"] <- "weekend"
```

There were 35 days’ observation, and 1440 activies per day.

``` r
sheet_data["total_activity"] <- rowSums(sheet_data[ ,4:1443])  

show_trend = sheet_data %>% 
  select(week, day_id, total_activity)

show_trend %>% 
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_line()
```

![](p8105_hw3_ql2370_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
The trend is unconstant from the plot.

``` r
plot_df = 
  pivot_longer(
    sheet_data, 
    activity_1:activity_1440,
    names_to = "activity", 
    values_to = "accel_data")


plot_df %>% 
  ggplot(aes(x = activity, y = accel_data, color = week)) +
  geom_line()
```

![](p8105_hw3_ql2370_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

From this plot, I found that, in the five weeks, the accelerometer data
started ok, and then increased rapidly. After that, it went lower again
in the first half of acities.

Also, I found that the higher figures were not from week 5 and 4.
